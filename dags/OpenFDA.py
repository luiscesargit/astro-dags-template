"""
OpenFDA
DAG auto-generated by Astro Cloud IDE.
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import requests

# ====== CONFIG ======
GCP_PROJECT  = "linen-walker-470814-e8"    # e.g., "my-gcp-project"
BQ_DATASET   = "crypto"                    # e.g., "crypto"
BQ_TABLE     = "fda_history_hourly"        # e.g., "bitcoin_history_hourly"
BQ_LOCATION  = "US"                        # dataset location: "US" or "EU"
GCP_CONN_ID  = "google_cloud_default"      # Airflow connection with a SA that can write to BQ
# ====================

# Function to generate query URL for a specific month and year
def generate_query_url(year, month):
    start_date = f"{year}{month:02d}01"
    end_date = f"{year}{month:02d}{(datetime(year, month, 1) + timedelta(days=31)).replace(day=1) - timedelta(days=1):%d}"
    query = f"https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:%22sildenafil+citrate%22+AND+receivedate:[{start_date}+TO+{end_date}]&count=receivedate"
    return query

# Function to fetch data from the API and save it to XCom
def fetch_openfda_data(ds, ti, **context):
    from airflow.operators.python import get_current_context
    context = get_current_context()
    execution_date = context['dag_run'].execution_date
    year = execution_date.year
    month = execution_date.month
    
    query_url = generate_query_url(year, month)
    response = requests.get(query_url)
    
    if response.status_code == 200:
        data = response.json()
        df = pd.DataFrame(data['results'])
        df['time'] = pd.to_datetime(df['time'])
        
        # Group by week and sum the count column
        weekly_sum = df.groupby(pd.Grouper(key='time', freq='W'))['count'].sum().reset_index()
        weekly_sum["time"] = weekly_sum["time"].astype(str)
        
        print(weekly_sum.info())
        print(weekly_sum.head())
    else:
        weekly_sum = pd.DataFrame([])  # Return empty DataFrame if request fails
    
    # Push the DataFrame to XCom
    ti.xcom_push(key='openfda_data', value=weekly_sum.to_dict())

def save_to_bigquery(ds, ti, **context):
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from google.cloud import bigquery
    
    # Retrieve the DataFrame from XCom
    data_dict = ti.xcom_pull(task_ids='fetch_openfda_data', key='openfda_data')
    
    if data_dict and len(data_dict) > 0:
        df = pd.DataFrame.from_dict(data_dict)
        
        # Get BigQuery hook and client
        bq_hook = BigQueryHook(
            gcp_conn_id=GCP_CONN_ID,
            location=BQ_LOCATION
        )
        
        # Get client from hook
        client = bq_hook.get_client()
        
        # Define table reference
        table_ref = f"{GCP_PROJECT}.{BQ_DATASET}.{BQ_TABLE}"
        
        # Configure job to append data and auto-detect schema
        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_APPEND",
            autodetect=True,
            source_format=bigquery.SourceFormat.PARQUET
        )
        
        try:
            # Load DataFrame to BigQuery
            job = client.load_table_from_dataframe(
                df, 
                table_ref, 
                job_config=job_config
            )
            
            # Wait for the job to complete
            job.result()
            
            print(f"Successfully loaded {len(df)} rows into {table_ref}")
            
        except Exception as e:
            print(f"Error loading data to BigQuery: {str(e)}")
            raise e
    else:
        print("No data to load into BigQuery")

# Define the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'fetch_openfda_data_monthly',
    default_args=default_args,
    description='Retrieve OpenFDA data monthly and load to BigQuery',
    schedule='@monthly',
    start_date=datetime(2020, 11, 1),
    catchup=True,
    max_active_tasks=1
)

fetch_data_task = PythonOperator(
    task_id='fetch_openfda_data',
    python_callable=fetch_openfda_data,
    dag=dag,
)

save_data_task = PythonOperator(
    task_id='save_to_bigquery',
    python_callable=save_to_bigquery,
    dag=dag,
)

fetch_data_task >> save_data_task
